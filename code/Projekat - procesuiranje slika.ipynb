{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = \"../data/Train.csv\"\n",
    "testPath = \"../data/Test.csv\"\n",
    "metaPath = \"../data/Meta.csv\"\n",
    "\n",
    "trainCSV = pd.read_csv(trainPath)\n",
    "testCSV = pd.read_csv(testPath)\n",
    "metaCSV = pd.read_csv(metaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>ShapeId</th>\n",
       "      <th>ColorId</th>\n",
       "      <th>SignId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta/27.png</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta/0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meta/1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meta/10.png</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meta/11.png</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path  ClassId  ShapeId  ColorId SignId\n",
       "0  Meta/27.png       27        0        0   1.32\n",
       "1   Meta/0.png        0        1        0   3.29\n",
       "2   Meta/1.png        1        1        0   3.29\n",
       "3  Meta/10.png       10        1        0   3.27\n",
       "4  Meta/11.png       11        0        0   1.22"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaCSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = trainCSV[['Width', 'Height', 'ClassId', 'Path']]\n",
    "testCSV = testCSV[['Width', 'Height', 'ClassId', 'Path']]\n",
    "metaCSV = metaCSV[['Path', 'ClassId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToData = \"../data/\"\n",
    "targetPictureSize = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_images = []\n",
    "image_classes = []\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in trainCSV.iterrows():\n",
    "    # Extract the file path and class label from the DataFrame\n",
    "    file_path = pathToData + row[\"Path\"]  # Replace \"file_path_column_name\" with your actual column name\n",
    "    class_label = int(row[\"ClassId\"])  # Replace \"class_column_name\" with your actual column name\n",
    "    \n",
    "    # Check if the file path is valid and the file is a PNG image\n",
    "    if os.path.isfile(file_path) and file_path.endswith(\".png\"):\n",
    "        # Open the image\n",
    "        image = Image.open(file_path)\n",
    "        \n",
    "        # Convert the image to grayscale\n",
    "        grayscale_image = image.convert(\"L\")\n",
    "        \n",
    "        # Resize the grayscale image to 32x32 pixels\n",
    "        resized_image = grayscale_image.resize(targetPictureSize)\n",
    "        \n",
    "        # Transform the resized image into a PyTorch tensor\n",
    "        transform = torchvision.transforms.ToTensor()\n",
    "        tensor_image = transform(resized_image)\n",
    "        converted_tensor = tensor_image.permute(1, 2, 0)\n",
    "        \n",
    "        # Add the tensor and its corresponding class label to the lists\n",
    "        resized_images.append(converted_tensor)\n",
    "        image_classes.append(class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_image = resized_images[211]\n",
    "\n",
    "numpy_image = example_image.squeeze().numpy()\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(numpy_image, cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39209])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    labels_tensor = torch.tensor(image_classes)\n",
    "    print(labels_tensor.shape)\n",
    "except:\n",
    "    print(\"Error while stacking the images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 43\u001B[0m\n\u001B[1;32m     40\u001B[0m noOfClasses \u001B[39m=\u001B[39m \u001B[39m43\u001B[39m\n\u001B[1;32m     42\u001B[0m \u001B[39m# Create an instance of your model\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m model \u001B[39m=\u001B[39m RegularNN(no_Of_Filters, size_of_Filter, size_of_Filter2, size_of_pool, no_Of_Nodes, noOfClasses)\n\u001B[1;32m     44\u001B[0m \u001B[39m# Print the model summary\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[39mprint\u001B[39m(model)\n",
      "Cell \u001B[0;32mIn[48], line 5\u001B[0m, in \u001B[0;36mRegularNN.__init__\u001B[0;34m(self, no_Of_Filters, size_of_Filter, size_of_Filter2, size_of_pool, no_Of_Nodes, noOfClasses)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m__init__\u001B[39m(\u001B[39mself\u001B[39m, no_Of_Filters, size_of_Filter, size_of_Filter2, size_of_pool, no_Of_Nodes, noOfClasses):\n\u001B[1;32m      3\u001B[0m     \u001B[39msuper\u001B[39m(RegularNN, \u001B[39mself\u001B[39m)\u001B[39m.\u001B[39m\u001B[39m__init__\u001B[39m()\n\u001B[0;32m----> 5\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mconv1 \u001B[39m=\u001B[39m nn\u001B[39m.\u001B[39;49mConv2d(no_Of_Filters, size_of_Filter)\n\u001B[1;32m      6\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mconv2 \u001B[39m=\u001B[39m nn\u001B[39m.\u001B[39mConv2d(no_Of_Filters, no_Of_Filters, size_of_Filter)\n\u001B[1;32m      7\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpool \u001B[39m=\u001B[39m nn\u001B[39m.\u001B[39mMaxPool2d(size_of_pool)\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() missing 1 required positional argument: 'kernel_size'"
     ]
    }
   ],
   "source": [
    "class RegularNN(nn.Module):\n",
    "    def __init__(self, no_Of_Filters, size_of_Filter, size_of_Filter2, size_of_pool, no_Of_Nodes, noOfClasses):\n",
    "        super(RegularNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, no_Of_Filters, size_of_Filter)\n",
    "        self.conv2 = nn.Conv2d(no_Of_Filters, no_Of_Filters, size_of_Filter)\n",
    "        self.pool = nn.MaxPool2d(size_of_pool)\n",
    "        self.conv3 = nn.Conv2d(no_Of_Filters, no_Of_Filters // 2, size_of_Filter2)\n",
    "        self.conv4 = nn.Conv2d(no_Of_Filters // 2, no_Of_Filters // 2, size_of_Filter2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(no_Of_Filters // 2 * 8 * 8, no_Of_Nodes)\n",
    "        self.fc2 = nn.Linear(no_Of_Nodes, noOfClasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the model parameters\n",
    "no_Of_Filters = 60\n",
    "size_of_Filter = 5\n",
    "size_of_Filter2 = 3\n",
    "size_of_pool = 2\n",
    "no_Of_Nodes = 500\n",
    "noOfClasses = 43\n",
    "\n",
    "# Create an instance of your model\n",
    "model = RegularNN(no_Of_Filters, size_of_Filter, size_of_Filter2, size_of_pool, no_Of_Nodes, noOfClasses)\n",
    "# Print the model summary\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Convert your lists of images and labels to tensors\n",
    "images_tensor = torch.stack(resized_images)\n",
    "labels_tensor = torch.tensor(image_classes)\n",
    "\n",
    "# Create an instance of your dataset\n",
    "dataset = TrainDataset(images_tensor, labels_tensor)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a data loader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [60, 1, 5, 5], expected input[32, 32, 32, 1] to have 1 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m optimizer\u001B[39m.\u001B[39mzero_grad()\n\u001B[1;32m     12\u001B[0m \u001B[39m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m outputs \u001B[39m=\u001B[39m model(images)\n\u001B[1;32m     14\u001B[0m loss \u001B[39m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     16\u001B[0m \u001B[39m# Backward pass and optimize\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[45], line 15\u001B[0m, in \u001B[0;36mRegularNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, x):\n\u001B[0;32m---> 15\u001B[0m     x \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mconv1(x)\n\u001B[1;32m     16\u001B[0m     x \u001B[39m=\u001B[39m nn\u001B[39m.\u001B[39mReLU()(x)\n\u001B[1;32m     17\u001B[0m     x \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mconv2(x)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39minput\u001B[39m: Tensor) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_conv_forward(\u001B[39minput\u001B[39;49m, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mweight, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mbias)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpadding_mode \u001B[39m!=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mzeros\u001B[39m\u001B[39m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[39mreturn\u001B[39;00m F\u001B[39m.\u001B[39mconv2d(F\u001B[39m.\u001B[39mpad(\u001B[39minput\u001B[39m, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[39m0\u001B[39m), \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdilation, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[39mreturn\u001B[39;00m F\u001B[39m.\u001B[39;49mconv2d(\u001B[39minput\u001B[39;49m, weight, bias, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mstride,\n\u001B[1;32m    460\u001B[0m                 \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mpadding, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdilation, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mgroups)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Given groups=1, weight of size [60, 1, 5, 5], expected input[32, 32, 32, 1] to have 1 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), \"trained_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
